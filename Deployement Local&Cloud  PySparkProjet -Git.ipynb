{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f9c307",
   "metadata": {},
   "source": [
    "# NOTEBOOK DEPLOYEMENT CLOUD D'UNE APPLICATION DE MISE EN OEUVRE DES CLUSTERS DE CALCUL PYSPARK( Jobs Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a31143",
   "metadata": {},
   "source": [
    "# 1 Introduction;\n",
    "\n",
    "L'objet de ce projet est de deployer une application Spark jobs dans le cloud sur une plateforme Amazon Web Service.\n",
    "La conception de ce projet necessite de suivre la démarche analytique suivante:\n",
    "\n",
    "# a_Etablir une  surveillance du temps de calcul sur l'environnement local en vue d'une comparaison avec les performances en temps de l'environnement cloud.\n",
    "\n",
    "# b_Urbaniser la localisation  des données d'entrée et de sortie sur les differents disques\n",
    "    \n",
    "\n",
    "# c_Charger les données\n",
    "      Explorez Spark Dataframe au format 'image'\n",
    "      Nous garderons l'origine pour obtenir le chemin et extraire l'étiquette\n",
    "      Autre façon de traiter l'image\n",
    "    \n",
    "# d_Utiliser un CNN comme extracteur de fonctionnalités\n",
    "\n",
    "###   ===>   Faire une caractérisation  des images :\n",
    "        Définir la logique de chargement et de caractéristiques des images dans une UDF Pandas\n",
    "    \n",
    "###  ===> Procéder à une réduction dimensionnelle\n",
    "        Transformer des tableaux en vecteurs pour effectuer une réduction\n",
    "        Initialiser et appliquer PCA\n",
    "        Transformation inverse : des vecteurs au tableau - c'est-à-dire la lisibilité des pandas\n",
    "            \n",
    "# c_Collecter et Sauvegarder les résultats en format parquet en vue d'une exploitation ultérieure.\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bcac26",
   "metadata": {},
   "source": [
    "# 2 Importation des librairies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4799a582",
   "metadata": {},
   "source": [
    "Dans ce cahier, nous gérons via pySpark, sql & ml, la caractérisation et la réduction de dimension d'une collection d'images.\n",
    "L'objectif est de construire une architecture évolutive pour effectuer les transformations attendues et permettre une croissance rapide de la collection d'images en gardant un temps de calcul viable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521ecf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "############################\n",
    "import sys\n",
    "import pyspark\n",
    "import findspark\n",
    "from platform import python_version\n",
    "############################\n",
    "#Packlages Pyspark\n",
    "############################\n",
    "from PIL import Image,ImageDraw\n",
    "from typing import Iterator\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from PIL import Image, ImageDraw\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split,udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType, BinaryType\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "import pyarrow\n",
    "#transform\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import *\n",
    "##################################\n",
    "#Packages Keras\n",
    "##################################\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array,load_img\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910b080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Bienvenue sur votre environnement de production Amazon Web Service pour jobs spark***********\n",
      "\n",
      "La version Python extrait de l environnement Aws_Pyspark est: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:14:58) [MSC v.1929 64 bit (AMD64)]\n",
      "La version de l'application Spark est: 3.3.2\n",
      "L'OS de l'environnement de deploiement est: win32\n",
      "\n",
      "PATH:        s3://myprojet8bucket\n",
      "PATH_DATA:   s3://myprojet8bucket/Input-Test/*\n",
      "PATH_FEATURES: s3://myprojet8bucket/Output/Features\n",
      "PATH_FINAL_FEATURES: s3://myprojet8bucket/Output/Final_Features\n"
     ]
    }
   ],
   "source": [
    "#Selection de l'environnement de deploiement\n",
    "Environnement_Test_Local=False\n",
    "    \n",
    "#===================================================================================================================    \n",
    "if Environnement_Test_Local:        \n",
    "    findspark.init()\n",
    "    # 4 Configuration des variables utilisateurs sur l'environnement de deploiement de Test après chargement de Python,Java et Spark dans le serveur windows \n",
    "\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable          \n",
    "\n",
    "    ## Urbanisation en local\n",
    "\n",
    "    PATH='C:\\\\Users\\\\dgama\\\\Desktop\\\\OPENCLASSROOMS_INFO\\\\REPERTOIRE_P8_BIS'\n",
    "\n",
    "    PATH_R='C:\\\\Users\\\\dgama\\\\Desktop\\\\OPENCLASSROOMS_INFO\\\\REPERTOIRE_P8_TEMP'\n",
    "\n",
    "    PATH_Data = PATH+'\\Test2'\n",
    "    PATH_Result = PATH_R+'\\Results'\n",
    "    print(\"********Bienvenue sur votre environnement de Test local pour jobs spark***********\"+\\\n",
    "         \"\\n\\nLa version Python extrait de l environnement Conda pyspark_env est:\"\\\n",
    "         ,sys.version +\"\\nLa version de l'application Spark est:\"\\\n",
    "         ,pyspark.__version__ +\"\\nL'OS de l'environnement de deploiement est:\"\\\n",
    "         ,sys.platform +\"\\n\\nPATH:        \"+\\\n",
    "         PATH+\"\\nPATH_Data:   \"+\\\n",
    "         PATH_Data +\"\\nPATH_Result:\"+ PATH_Result)\n",
    "#==========================================================================================================================       \n",
    "else:\n",
    "   \n",
    "    # 4 Configuration des variables utilisateurs sur l'environnement de deploiement de Test après chargement de Python,Java et Spark dans le serveur windows \n",
    "\n",
    "    ## Urbanisation en local\n",
    "\n",
    "    PATH='s3://myprojet8bucket'\n",
    "\n",
    "    PATH_DATA='s3://myprojet8bucket/Input-Test/*'\n",
    "\n",
    "    PATH_FEATURES='s3://myprojet8bucket/Output_Data/features/'\n",
    "\n",
    "    PATH_FINAL_FEATURES='s3://myprojet8bucket/Output_Data/final_features/'\n",
    "\n",
    "    PATH ='s3://myprojet8bucket'\n",
    "    PATH_Data = PATH+'/Input-Test/*'\n",
    "    PATH_FEATURES = PATH+'/Output/Features'\n",
    "    PATH_FINAL_FEATURES=PATH+'/Output/Final_Features'\n",
    "    print(\"********Bienvenue sur votre environnement EMR Notebook _Amazon Web Service pour jobs spark***********\"+\\\n",
    "         \"\\n\\nLa version Python extrait de l environnement Aws_Pyspark est:\"\\\n",
    "         ,sys.version +\"\\nLa version de l'application Spark est:\"\\\n",
    "         ,pyspark.__version__ +\"\\nL'OS de l'environnement de deploiement est:\"\\\n",
    "         ,sys.platform +\"\\n\\nPATH:        \"+\\\n",
    "          PATH+\"\\nPATH_DATA:   \"+\\\n",
    "          PATH_Data+\"\\nPATH_FEATURES: \"\\\n",
    "          +PATH_FEATURES +\"\\nPATH_FINAL_FEATURES: \"\\\n",
    "           +PATH_FINAL_FEATURES)\n",
    "\n",
    "#================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Session Spark\n",
    "\n",
    "spark = (SparkSession\n",
    "             .builder\n",
    "             .appName('P8')\n",
    "             .master('local')\n",
    "             .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "             .getOrCreate()\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wether arrow should be enabled by this setting\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99970ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open spark UI for app monitoring\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load data into into a spark_d\n",
    "df_img = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaadd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of images in this sample\n",
    "df_img.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503489f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Path de l'image conservé et nous ajoutons une colonne contenant les labels de chaque image\n",
    "\n",
    "df_img = df_img.withColumn('label', element_at(split(df_img['path'], '/'),-2))\n",
    "print(df_img.printSchema())\n",
    "print(df_img.select('path','label').show(5,False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aceb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 Préparation du Modèle\n",
    "\n",
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224,224,3))\n",
    "\n",
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights as broadcasted variable over nodes (provide a copy to each node)\n",
    "\n",
    "brodcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 Caractérisation (Featurizer)  et Reduction dimensionnelle(Reducer)\n",
    "'''\n",
    "Nous utilisons le transfer learning pour featuriser les images .\n",
    "Nous allons procéder à la réduction des la dimension des torseurs d'images en transformons les matrices d'images\n",
    "en Vecteurs d'images'''\n",
    "\n",
    "start = time.perf_counter()\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763003e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09401498",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_df = df_img.repartition(20).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )\n",
    "\n",
    "featurized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "featurized_df.write.mode(\"overwrite\").parquet(PATH_Result)\n",
    "\n",
    "featurized_dt = pd.read_parquet(PATH_Result, engine='pyarrow')\n",
    "\n",
    "featurized_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3391c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of paritions\n",
    "\n",
    "print(featurized_df.rdd.getNumPartitions())\n",
    "\n",
    "### On valide que les vecteur des caracterisque des images sint  bien de dimension 1280\n",
    "featurized_dt.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Array to Vectors for PCA\n",
    "\n",
    "array_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "vectorized_df = featurized_df.withColumn('cnn_vectors', array_to_vector_udf('features'))\n",
    "\n",
    "vectorized_df.show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb22936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Initialisation et Application PCA\n",
    "\n",
    "# Results may vary with larger dataset, creates an heavy task Action that would affect overall performance.\n",
    "\n",
    "# reduce with PCA - set k Max to determine the adequate nb of principal components\n",
    "\n",
    "pca = PCA(k=20, inputCol='cnn_vectors', outputCol='pca_vectors')\n",
    "model = pca.fit(vectorized_df)\n",
    "\n",
    "# apply pca reduction\n",
    "\n",
    "reduced_df = model.transform(vectorized_df)\n",
    "\n",
    "reduced_df.show(5, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04263455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 Transformation inverse: Des vecteurs vers un tableau - i.e. DataFrame Pandas\n",
    "\n",
    "# from Array to Vectors for PCA\n",
    "vector_to_array_udf = udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))\n",
    "\n",
    "final_df = reduced_df.withColumn('features', vector_to_array_udf('pca_vectors'))\n",
    "\n",
    "final_df.show(5, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 Collectes et Sauvegardes finales des Jobs Spark\n",
    "\n",
    "# write local results on parquet file\n",
    "\n",
    "final_df.write.mode('overwrite').parquet(PATH_Result)\n",
    "\n",
    "# read local results from parquet file\n",
    "\n",
    "pd_final_df = pd.read_parquet(PATH_Result, engine='pyarrow')\n",
    "\n",
    "# size of the results df\n",
    "\n",
    "pd_final_df.info(verbose=False, memory_usage=\"deep\")\n",
    "\n",
    "# overview\n",
    "pd_final_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc12f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fin du processus Job spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324060a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
